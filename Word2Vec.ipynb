{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0bMYy45NL2E",
        "outputId": "63fb38aa-ff30-4abf-c8cf-527e64911329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wikidata in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (0.8.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wikidata\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDhz6g8cNwvu",
        "outputId": "298d8878-4b58-421f-8f7c-6653171fabec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"from google.colab import drive\\ndrive.mount('/content/drive/', force_remount=True)\\ncheckpoint_path = '/content/drive/MyDrive/NLP_HOMEWORK_1/word2vec/'\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "checkpoint_path = '/content/drive/MyDrive/NLP_HOMEWORK_1/word2vec/'\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1Doi5gsJOWrX",
        "outputId": "2ee7a5b4-f37b-4d3b-9189-a1d76a9ecd3c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item</th>\n",
              "      <th>name</th>\n",
              "      <th>description</th>\n",
              "      <th>type</th>\n",
              "      <th>category</th>\n",
              "      <th>subcategory</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Q306</td>\n",
              "      <td>Sebastián Piñera</td>\n",
              "      <td>Chilean entrepreneur and politician (1949–2024)</td>\n",
              "      <td>entity</td>\n",
              "      <td>politics</td>\n",
              "      <td>politician</td>\n",
              "      <td>cultural exclusive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Q12735</td>\n",
              "      <td>John Amos Comenius</td>\n",
              "      <td>Czech teacher, educator, philosopher and write...</td>\n",
              "      <td>entity</td>\n",
              "      <td>politics</td>\n",
              "      <td>politician</td>\n",
              "      <td>cultural representative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Q1752</td>\n",
              "      <td>Macrinus</td>\n",
              "      <td>Roman emperor from 217 to 218</td>\n",
              "      <td>entity</td>\n",
              "      <td>politics</td>\n",
              "      <td>politician</td>\n",
              "      <td>cultural representative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Q1639</td>\n",
              "      <td>Lamine Diack</td>\n",
              "      <td>Senegalese sports manager (1933–2021)</td>\n",
              "      <td>entity</td>\n",
              "      <td>politics</td>\n",
              "      <td>politician</td>\n",
              "      <td>cultural representative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Q9588</td>\n",
              "      <td>Richard Nixon</td>\n",
              "      <td>President of the United States from 1969 to 1974</td>\n",
              "      <td>entity</td>\n",
              "      <td>politics</td>\n",
              "      <td>politician</td>\n",
              "      <td>cultural representative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     item                name  \\\n",
              "0    Q306    Sebastián Piñera   \n",
              "1  Q12735  John Amos Comenius   \n",
              "2   Q1752            Macrinus   \n",
              "3   Q1639        Lamine Diack   \n",
              "4   Q9588       Richard Nixon   \n",
              "\n",
              "                                         description    type  category  \\\n",
              "0    Chilean entrepreneur and politician (1949–2024)  entity  politics   \n",
              "1  Czech teacher, educator, philosopher and write...  entity  politics   \n",
              "2                      Roman emperor from 217 to 218  entity  politics   \n",
              "3              Senegalese sports manager (1933–2021)  entity  politics   \n",
              "4   President of the United States from 1969 to 1974  entity  politics   \n",
              "\n",
              "  subcategory                    label  \n",
              "0  politician       cultural exclusive  \n",
              "1  politician  cultural representative  \n",
              "2  politician  cultural representative  \n",
              "3  politician  cultural representative  \n",
              "4  politician  cultural representative  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_path = \"./gold_dataset_clean.csv\"\n",
        "dataset = pd.read_csv(dataset_path, sep=\",\")\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDYeLX93O3uo",
        "outputId": "a25875ab-9344-44fc-bb04-aedf036530a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6238, 7)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "UNpDR9xrO5Ff",
        "outputId": "e986d794-db1a-492b-fb1e-8d0dda60ff0b"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "from wikidata.client import Client\n",
        "\n",
        "\n",
        "wiki_path = \"/content/drive/MyDrive/NLP_HOMEWORK_1/data/wikitext.txt\"\n",
        "\n",
        "def retrieve_wikitext(entities_id, file_path):\n",
        "  \"\"\"\n",
        "    Function to return Wikipedia Article of a given Wikidata Entity (in english).\n",
        "\n",
        "    Arguments:\n",
        "    entities_id -- list of entities id from Wikidata\n",
        "  \"\"\"\n",
        "  # Wikidata client instantiation\n",
        "  client = Client()\n",
        "\n",
        "\n",
        "\n",
        "  # Useful sub-functions\n",
        "  def clean_wikipedia_extract(text):\n",
        "    \"\"\"Sub-function that cleans wikipedia text\"\"\"\n",
        "\n",
        "    # Remove unwanted paragraphs\n",
        "\n",
        "    text = re.sub(r\"^==.*?==\\s*\", \"\", text, flags=re.MULTILINE)\n",
        "\n",
        "    end_markers = [\"See also\", \"References\", \"External links\", \"Further reading\"]\n",
        "    for marker in end_markers:\n",
        "        pattern = rf\"==\\s*{marker}\\s*==.*\"\n",
        "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"\\t\", \" \")\n",
        "    return text.strip()\n",
        "\n",
        "  def get_text(item):\n",
        "    \"\"\"\n",
        "      Sub-function that handles the get request from Wikipedia\n",
        "\n",
        "      Arguments:\n",
        "      item -- wikidata.Entity\n",
        "    \"\"\"\n",
        "    sitelinks = item.data.get(\"sitelinks\", {})\n",
        "    enwiki = sitelinks.get(\"enwiki\")\n",
        "    if enwiki:\n",
        "        title = enwiki[\"title\"]\n",
        "\n",
        "        api_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "        params = {\n",
        "            \"action\": \"query\",\n",
        "            \"prop\": \"extracts\",\n",
        "            \"explaintext\": True,\n",
        "            \"titles\": title,\n",
        "            \"format\": \"json\",\n",
        "            \"redirects\": 1\n",
        "        }\n",
        "\n",
        "        res = requests.get(api_url, params=params).json()\n",
        "        pages = res.get(\"query\", {}).get(\"pages\", {})\n",
        "        if not pages:\n",
        "            return \"\"\n",
        "        page = next(iter(pages.values()))\n",
        "        text = page.get(\"extract\", \"\")\n",
        "        text = text.lower()\n",
        "        text = clean_wikipedia_extract(text)\n",
        "        return text\n",
        "    else:\n",
        "      print(f\"No English Wikipedia page found for entity . (skipping)\")\n",
        "      return \"\"\n",
        "\n",
        "\n",
        "  tot = len(entities_id)\n",
        "  with open(file_path, \"a\") as f:\n",
        "    for entity_id in tqdm(entities_id, total=tot):\n",
        "      item = client.get(entity_id, load=True)\n",
        "      text = get_text(item)\n",
        "      text += \"\\n\"\n",
        "      f.write(text)\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retrieve text from all articles (78m 34s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "entities_id = dataset[\"item\"].to_numpy()\n",
        "\n",
        "if(not os.path.isfile(\"./wiki-text.txt\")):\n",
        "    retrieve_wikitext(entities_id=entities_id, file_path=\"./wiki-text.txt\")\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:87: SyntaxWarning: invalid escape sequence '\\W'\n",
            "<>:87: SyntaxWarning: invalid escape sequence '\\W'\n",
            "/tmp/ipykernel_3380/784581134.py:87: SyntaxWarning: invalid escape sequence '\\W'\n",
            "  def tokenize_line(self, line, pattern='\\W'):\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "\n",
        "class Word2VecDataset(torch.utils.data.IterableDataset):\n",
        "\n",
        "    def __init__(self, txt_path, vocab_size, unk_token, window_size, pre_word2id=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          txt_file (str): Path to the raw-text file.\n",
        "          vocab_size (int): Maximum amount of words that we want to embed.\n",
        "          unk_token (str): How will unknown words represented (e.g. 'UNK').\n",
        "          window_size (int): Number of words to consider as context.\n",
        "          pre_word2id (np.array): Word to ID dictionary of a pretrained model \n",
        "        \"\"\"\n",
        "        self.window_size = window_size\n",
        "        self.pre_word2id = pre_word2id\n",
        "        # [[w_{1,s1}, w_{2,s1}, ..., w_{|s1|,s1}], [w_{1,s2}, w_{2,s2}, ..., w_{|s2|,s2}], ..., [w_{1,sn}, ..., w_{|sn|,sn}]]\n",
        "        self.data_words = self.read_data(txt_path)\n",
        "        self.build_vocabulary(vocab_size, unk_token)\n",
        "\n",
        "        \n",
        "\n",
        "    def __iter__(self):\n",
        "\n",
        "        \"\"\"\n",
        "          When iterating through this dataset we must include the window size\n",
        "          So for each sentence in the dataset we iterate through each word\n",
        "\n",
        "          For each word wi we output input: wi, target: wi-j\n",
        "          where j is in range (-window_size, window_size)\n",
        "\n",
        "          Example:\n",
        "          sentence = [\"the\", \"dog\", \"is\", \"lazy\"]\n",
        "          window_size = 2\n",
        "\n",
        "          first iteration -> {input: \"the\" (as index), output: \"dog\"} {input: \"the\", output: \"is\"}\n",
        "\n",
        "        \"\"\"\n",
        "        sentences = self.data_words\n",
        "        for sentence in sentences:\n",
        "            len_sentence = len(sentence)\n",
        "\n",
        "            for input_idx in range(len_sentence):\n",
        "                current_word = sentence[input_idx]\n",
        "\n",
        "                # must be a word in the vocabulary\n",
        "                if current_word in self.word2id and self.keep_word(current_word):\n",
        "                    # index of input word\n",
        "                    current_word_id = self.word2id[current_word]\n",
        "\n",
        "                    # left and right window indices\n",
        "                    min_idx = max(0, input_idx - self.window_size)\n",
        "                    max_idx = min(len_sentence, input_idx + self.window_size)\n",
        "\n",
        "                    window_idxs = [x for x in range(min_idx, max_idx) if x != input_idx]\n",
        "                    for target_idx in window_idxs:\n",
        "                        # must be a word in the vocabulary\n",
        "                        if sentence[target_idx] in self.word2id:\n",
        "                            # index of target word in vocab\n",
        "                            target_word_id = self.word2id[sentence[target_idx]]\n",
        "                            output_dict = {'inputs':current_word_id, 'targets':target_word_id}\n",
        "\n",
        "                            yield output_dict\n",
        "\n",
        "    def keep_word(self, word):\n",
        "        '''Implements subsampling to avoid overly frequent words and returns true if we can keep the occurrence as training instance.'''\n",
        "        z = self.frequency[word] / self.tot_occurrences  # f(w): relative frequency\n",
        "        t = 1e-5  # standard value used in practice\n",
        "        p_keep = np.sqrt(t / z)\n",
        "        p_keep = min(1.0, p_keep)  # cap at 1\n",
        "        return np.random.rand() < p_keep\n",
        "\n",
        "    def read_data(self, txt_path):\n",
        "        \"\"\"Converts each line in the input file into a list of lists of tokenized words.\"\"\"\n",
        "        data = []\n",
        "        total_words = 0\n",
        "        # tot_lines = self.count_lines(txt_path)\n",
        "        with open(txt_path) as f:\n",
        "            for line in f:\n",
        "                split = self.tokenize_line(line)\n",
        "                if split:\n",
        "                    # split is a list of words which is appended to data\n",
        "                    data.append(split)\n",
        "                    total_words += len(split)\n",
        "        return data\n",
        "\n",
        "    # \"The pen is on the table\" -> [\"the, \"pen\", \"is\", \"on\", \"the\", \"table\"]\n",
        "    def tokenize_line(self, line, pattern='\\W'):\n",
        "        \"\"\"Tokenizes a single line.\"\"\"\n",
        "\n",
        "        return [word for word in re.split(pattern, line.lower()) if word]\n",
        "\n",
        "    def build_vocabulary(self, vocab_size, unk_token):\n",
        "        \"\"\"Defines the vocabulary to be used. Builds a mapping (word, index) for\n",
        "        each word in the vocabulary.\n",
        "\n",
        "        Args:\n",
        "          vocab_size (int): size of the vocabolary\n",
        "          unk_token (str): token to associate with unknown words\n",
        "        \"\"\"\n",
        "        counter_list = []\n",
        "        # context is a list of tokens within a single sentence\n",
        "        for context in self.data_words:\n",
        "            counter_list.extend(context)\n",
        "        counter = collections.Counter(counter_list)\n",
        "        # just for debugging/example purposes: print(counter['house'], counter['plane'], counter['the'])\n",
        "        counter_len = len(counter)\n",
        "        print(f\"Number of distinct words: {counter_len}\")\n",
        "\n",
        "        # consider only the (vocab size-1) most common words to build the vocab\n",
        "        # dictionary will contain pairs 'word, index' where index is a unique ID for the word\n",
        "        # which will be used as the position in the one-hot encoding for the word\n",
        "\n",
        "\n",
        "        #Compatibility with pre trained models \n",
        "        if(self.pre_word2id != None):\n",
        "            self.word2id = self.pre_word2id \n",
        "            word2index = self.pre_word2id\n",
        "        else:\n",
        "            word2index = {key: index for index, (key, _) in enumerate(counter.most_common(vocab_size-1))}\n",
        "            # UNK doesn't occur in the dictionary\n",
        "            assert unk_token not in word2index\n",
        "            # all the other words are mapped to UNK\n",
        "            word2index[unk_token] = vocab_size-1\n",
        "            self.word2id = word2index\n",
        "\n",
        "        # we create a new \"counter\" dictionary only for our vocab words, containing (word, frequency) pairs\n",
        "        dict_counts = {x: counter[x] for x in word2index if x is not unk_token}\n",
        "        self.frequency = dict_counts\n",
        "        self.tot_occurrences = sum(dict_counts[x] for x in dict_counts)\n",
        "\n",
        "        print(f'Total occurrences of words in the dataset (excl. UNK tokens): {self.tot_occurrences}')\n",
        "\n",
        "        if(self.pre_word2id == None):\n",
        "            less_freq_word = min(dict_counts, key=counter.get)\n",
        "            print(f'Least frequent word in dictionary ({less_freq_word}) appears {dict_counts[less_freq_word]} times')\n",
        "\n",
        "        # create the index to word dictionary\n",
        "\n",
        "        self.id2word = {value: key for key, value in word2index.items()}\n",
        "\n",
        "        # data is the text converted to indexes, as list of lists\n",
        "        data = []\n",
        "        # for each sentence\n",
        "        for sentence in self.data_words:\n",
        "            paragraph = []\n",
        "            # for each word in the sentence\n",
        "            for w in sentence:\n",
        "                #retrieve the id of the word\n",
        "                #if the word is an unknown word, don't add to data list\n",
        "                #otherwise it may cause inbalance\n",
        "                id_ = word2index[w] if w in word2index else word2index[unk_token]\n",
        "                if id_ == word2index[unk_token]:\n",
        "                    continue\n",
        "                paragraph.append(id_)\n",
        "            data.append(paragraph)\n",
        "        # list of lists of indices, where each sentence is a list of indices, ignoring UNK\n",
        "        self.data_idx = data\n",
        "        # now I have a field self.data_words which contains the sentences as\n",
        "        # a list of lists of tokens and a field self.data_idx which contains\n",
        "        # the sentences as a list of lists of the corresponding indices having\n",
        "        # removed unknown tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of distinct words: 268036\n",
            "Total occurrences of words in the dataset (excl. UNK tokens): 9928675\n",
            "Least frequent word in dictionary (fix) appears 93 times\n"
          ]
        }
      ],
      "source": [
        "VOCAB_SIZE = 10000\n",
        "\n",
        "train_set = Word2VecDataset(\"./wiki-text.txt\", VOCAB_SIZE, \"UNK\", 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainset_loader = DataLoader(train_set, 32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SkipGram(nn.Module):\n",
        "\n",
        "    def __init__(self, vocabulary_size, embedding_dim, id2word, word_counts, weights=None):\n",
        "        super(SkipGram, self).__init__()\n",
        "        \n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # matrix W\n",
        "        # because the first component is just a lookup table, we avoid using: self.embeddings = nn.Linear(self.vocabulary_size, self.embedding_dim)\n",
        "        \n",
        "        if(weights != None):\n",
        "            # load pre-trained embedding\n",
        "            self.embeddings = nn.Embedding.from_pretrained(weights, freeze=False)\n",
        "        else:\n",
        "            self.embeddings = nn.Embedding(num_embeddings=vocabulary_size, embedding_dim=embedding_dim)\n",
        "        # matrix W' and loss function\n",
        "        self.output_weights = nn.Linear(self.embedding_dim, self.vocabulary_size)\n",
        "        self.loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_idx):\n",
        "        input_embeddings = self.embeddings(input_idx)\n",
        "        output_logits = self.output_weights(input_embeddings)\n",
        "        return output_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "    def __init__(self, model, optimizer, device):\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        # starts requires_grad for all layers\n",
        "        self.model.train()  # we are using this model for training (some layers have different behaviours in train and eval mode)\n",
        "        self.model.to(self.device)  # move model to GPU if available\n",
        "\n",
        "    def train(self, train_dataset, output_folder, epochs=1, total_batches=0):\n",
        "\n",
        "        train_loss = 0.0\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            # each element (sample) in train_dataset is a batch\n",
        "            for step, sample in tqdm(enumerate(train_dataset), total=total_batches, desc=\"Batch\", leave=False):\n",
        "\n",
        "                inputs = sample['inputs'].to(self.device)\n",
        "                # outputs in the batch\n",
        "                targets = sample['targets'].to(self.device)\n",
        "\n",
        "                output_distribution = self.model(inputs)\n",
        "                loss = self.model.loss_function(output_distribution, targets)\n",
        "\n",
        "                # calculates the gradient and accumulates\n",
        "                loss.backward()  # we backpropagate the loss\n",
        "                # updates the parameters\n",
        "                # by applying the gradients to update all the parameters (weights and biases) of the model\n",
        "                # as computed using backpropagation during the call to loss.backward()\n",
        "                self.optimizer.step()\n",
        "                # zeroes the gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "            avg_epoch_loss = epoch_loss / num_batches\n",
        "\n",
        "            print(f'Epoch: {epoch} avg loss = {avg_epoch_loss:.4f}')\n",
        "\n",
        "            train_loss += avg_epoch_loss\n",
        "            torch.save(self.model.state_dict(),\n",
        "                       os.path.join(output_folder, f'state_{epoch}.pt'))  # save the model state\n",
        "\n",
        "        avg_epoch_loss = train_loss / epochs\n",
        "        return avg_epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of batches: 526824\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "model = SkipGram(vocabulary_size=VOCAB_SIZE, \n",
        "                 embedding_dim=300, \n",
        "                 id2word=train_set.id2word, \n",
        "                 word_counts=train_set.frequency)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "c = 0\n",
        "for b in trainset_loader:\n",
        "  c += 1\n",
        "print(\"Number of batches:\", c)\n",
        "\n",
        "trainer = Trainer(model, optimizer, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = False\n",
        "if(train):\n",
        "    avg_loss = trainer.train(trainset_loader, \"./data/word2vec/50k_vocab_size\", epochs=10, total_batches=c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for SkipGram:\n\tsize mismatch for embeddings.weight: copying a param with shape torch.Size([4000, 300]) from checkpoint, the shape in current model is torch.Size([50000, 300]).\n\tsize mismatch for output_weights.weight: copying a param with shape torch.Size([4000, 300]) from checkpoint, the shape in current model is torch.Size([50000, 300]).\n\tsize mismatch for output_weights.bias: copying a param with shape torch.Size([4000]) from checkpoint, the shape in current model is torch.Size([50000]).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m]:\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;66;03m## load model from checkpoint\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m   \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/word2vec/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;66;03m# set the model in evaluation mode\u001b[39;00m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;66;03m# (disables dropout, does not update parameters and gradient)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m   model\u001b[38;5;241m.\u001b[39meval()\n",
            "File \u001b[0;32m~/machine_learning_env/lib/python3.12/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SkipGram:\n\tsize mismatch for embeddings.weight: copying a param with shape torch.Size([4000, 300]) from checkpoint, the shape in current model is torch.Size([50000, 300]).\n\tsize mismatch for output_weights.weight: copying a param with shape torch.Size([4000, 300]) from checkpoint, the shape in current model is torch.Size([50000, 300]).\n\tsize mismatch for output_weights.bias: copying a param with shape torch.Size([4000]) from checkpoint, the shape in current model is torch.Size([50000])."
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "\n",
        "for epoch in [0, 5, 10]:\n",
        "  ## load model from checkpoint\n",
        "\n",
        "  model.load_state_dict(torch.load(os.path.join(\"./data/word2vec/\", f'state_{epoch}.pt')))\n",
        "\n",
        "  # set the model in evaluation mode\n",
        "  # (disables dropout, does not update parameters and gradient)\n",
        "  model.eval()\n",
        "\n",
        "  # retrieve the trained embeddings\n",
        "  embeddings = model.embeddings.weight\n",
        "\n",
        "  print(list(train_set.word2id.keys())[:100])\n",
        "\n",
        "  # pick some words to visualise\n",
        "  words = ['italy', 'rome', 'france', 'paris']\n",
        "\n",
        "  # perform PCA to reduce our 300d embeddings to 2d points that can be plotted\n",
        "  pca = PCA(n_components=2)\n",
        "  pca_result = pca.fit_transform(embeddings.detach().cpu()) # .t() transpose the embeddings\n",
        "\n",
        "  indexes = [train_set.word2id[x] for x in words]\n",
        "  \n",
        "  points = [pca_result[i] for i in indexes]\n",
        "  for i,(x,y) in enumerate(points):\n",
        "      plt.plot(x, y, 'ro')\n",
        "      plt.text(x, y, words[i], fontsize=12) # add a point label, shifted wrt to the point\n",
        "  plt.title('epoch {}'.format(epoch))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('puccini', tensor(0.2587, device='cuda:0', grad_fn=<SumBackward1>))"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def analogy_test(embeddings, positive, negative):\n",
        "    \"\"\"\n",
        "    Perform an analogy test: positive[0] - negative[0] + positive[1] ≈ closest word\n",
        "    \"\"\"\n",
        "    cosine_similarity = torch.nn.CosineSimilarity(dim=0)\n",
        "    pos_indexes = [train_set.word2id[word] for word in positive]\n",
        "    neg_indexes = [train_set.word2id[word] for word in negative]\n",
        "    # Get embeddings for the positive and negative words\n",
        "\n",
        "    positive_emb = 0\n",
        "    negative_emb = 0\n",
        "    for pos in pos_indexes:\n",
        "        positive_emb += embeddings[pos]\n",
        "    \n",
        "    for neg in neg_indexes:\n",
        "        negative_emb += embeddings[pos]\n",
        "    \n",
        "    # Calculate the resulting vector\n",
        "    result_vector = positive_emb - negative_emb\n",
        "    \n",
        "    # Find the most similar word in the vocabulary\n",
        "    max_similarity = -1\n",
        "    best_match_idx = -1\n",
        "    for idx in range(50000):\n",
        "        if idx in pos_indexes:  # Skip the words in the analogy itself\n",
        "            continue\n",
        "        if idx in neg_indexes:\n",
        "            continue\n",
        "\n",
        "        sim = cosine_similarity(result_vector, embeddings[idx])\n",
        "        if sim > max_similarity:\n",
        "            max_similarity = sim\n",
        "            best_match_idx = idx\n",
        "    \n",
        "    return train_set.id2word[best_match_idx], max_similarity\n",
        "\n",
        "\n",
        "word_a = 'pizza'\n",
        "word_b = 'italy'\n",
        "word_c = 'germany'\n",
        "\n",
        "word, sim = analogy_test(embeddings=embeddings, positive=[word_a, word_c], negative=[word_c])\n",
        "\n",
        "word, sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pre - Trained Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim.downloader\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "\n",
        "create_again = False \n",
        "if(create_again):\n",
        "    word2vec_google = gensim.downloader.load('word2vec-google-news-300')\n",
        "    pretrained_embedding = word2vec_google.vectors\n",
        "\n",
        "    pretrained_embedding = torch.tensor(pretrained_embedding)\n",
        "\n",
        "    torch.save(pretrained_embedding, \"word2vec_embedding.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Google's Word2Vec stuff\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['</s>',\n",
              " 'in',\n",
              " 'for',\n",
              " 'that',\n",
              " 'is',\n",
              " 'on',\n",
              " '##',\n",
              " 'The',\n",
              " 'with',\n",
              " 'said',\n",
              " 'was',\n",
              " 'the',\n",
              " 'at',\n",
              " 'not',\n",
              " 'as',\n",
              " 'it',\n",
              " 'be',\n",
              " 'from',\n",
              " 'by',\n",
              " 'are',\n",
              " 'I',\n",
              " 'have',\n",
              " 'he',\n",
              " 'will',\n",
              " 'has',\n",
              " '####',\n",
              " 'his',\n",
              " 'an',\n",
              " 'this',\n",
              " 'or',\n",
              " 'their',\n",
              " 'who',\n",
              " 'they',\n",
              " 'but',\n",
              " '$',\n",
              " 'had',\n",
              " 'year',\n",
              " 'were',\n",
              " 'we',\n",
              " 'more',\n",
              " '###',\n",
              " 'up',\n",
              " 'been',\n",
              " 'you',\n",
              " 'its',\n",
              " 'one',\n",
              " 'about',\n",
              " 'would',\n",
              " 'which',\n",
              " 'out',\n",
              " 'can',\n",
              " 'It',\n",
              " 'all',\n",
              " 'also',\n",
              " 'two',\n",
              " 'after',\n",
              " 'first',\n",
              " 'He',\n",
              " 'do',\n",
              " 'time',\n",
              " 'than',\n",
              " 'when',\n",
              " 'We',\n",
              " 'over',\n",
              " 'last',\n",
              " 'new',\n",
              " 'other',\n",
              " 'her',\n",
              " 'people',\n",
              " 'into',\n",
              " 'In',\n",
              " 'our',\n",
              " 'there',\n",
              " 'A',\n",
              " 'she',\n",
              " 'could',\n",
              " 'just',\n",
              " 'years',\n",
              " 'some',\n",
              " 'U.S.',\n",
              " 'three',\n",
              " 'million',\n",
              " 'them',\n",
              " 'what',\n",
              " 'But',\n",
              " 'so',\n",
              " 'no',\n",
              " 'like',\n",
              " 'if',\n",
              " 'only',\n",
              " 'percent',\n",
              " 'get',\n",
              " 'did',\n",
              " 'him',\n",
              " 'game',\n",
              " 'back',\n",
              " 'because',\n",
              " 'now',\n",
              " '#.#',\n",
              " 'before',\n",
              " 'company',\n",
              " 'any',\n",
              " 'team',\n",
              " 'against',\n",
              " 'off',\n",
              " 'This',\n",
              " 'most',\n",
              " 'made',\n",
              " 'through',\n",
              " 'make',\n",
              " 'second',\n",
              " 'state',\n",
              " 'well',\n",
              " 'day',\n",
              " 'season',\n",
              " 'says',\n",
              " 'week',\n",
              " 'where',\n",
              " 'while',\n",
              " 'down',\n",
              " 'being',\n",
              " 'government',\n",
              " 'your',\n",
              " '#-#',\n",
              " 'home',\n",
              " 'going',\n",
              " 'my',\n",
              " 'good',\n",
              " 'They',\n",
              " \"'re\",\n",
              " 'should',\n",
              " 'many',\n",
              " 'way',\n",
              " 'those',\n",
              " 'four',\n",
              " 'during',\n",
              " 'such',\n",
              " 'may',\n",
              " 'very',\n",
              " 'how',\n",
              " 'since',\n",
              " 'work',\n",
              " 'take',\n",
              " 'including',\n",
              " 'high',\n",
              " 'then',\n",
              " '%',\n",
              " 'next',\n",
              " '#,###',\n",
              " 'By',\n",
              " 'much',\n",
              " 'still',\n",
              " 'go',\n",
              " 'think',\n",
              " 'old',\n",
              " 'even',\n",
              " '#.##',\n",
              " 'world',\n",
              " 'see',\n",
              " 'say',\n",
              " 'business',\n",
              " 'five',\n",
              " 'told',\n",
              " 'under',\n",
              " 'us',\n",
              " '1',\n",
              " 'these',\n",
              " 'If',\n",
              " 'right',\n",
              " 'And',\n",
              " 'me',\n",
              " 'between',\n",
              " 'play',\n",
              " 'help',\n",
              " '##,###',\n",
              " 'market',\n",
              " 'That',\n",
              " 'know',\n",
              " 'end',\n",
              " 'AP',\n",
              " 'long',\n",
              " 'information',\n",
              " 'points',\n",
              " 'does',\n",
              " 'both',\n",
              " 'There',\n",
              " 'part',\n",
              " 'around',\n",
              " 'police',\n",
              " 'want',\n",
              " \"'ve\",\n",
              " 'based',\n",
              " 'For',\n",
              " 'got',\n",
              " 'third',\n",
              " 'school',\n",
              " 'left',\n",
              " 'another',\n",
              " 'country',\n",
              " 'need',\n",
              " '2',\n",
              " 'best',\n",
              " 'win',\n",
              " 'quarter',\n",
              " 'use',\n",
              " 'today',\n",
              " '##.#',\n",
              " 'same',\n",
              " 'public',\n",
              " 'run',\n",
              " 'Friday',\n",
              " 'set',\n",
              " 'month',\n",
              " 'top',\n",
              " 'billion',\n",
              " 'Tuesday',\n",
              " 'come',\n",
              " 'Monday',\n",
              " 'She',\n",
              " 'city',\n",
              " 'place',\n",
              " 'night',\n",
              " 'six',\n",
              " 'each',\n",
              " 'Thursday',\n",
              " '###,###',\n",
              " 'Wednesday',\n",
              " 'here',\n",
              " 'You',\n",
              " 'group',\n",
              " 'really',\n",
              " 'found',\n",
              " 'As',\n",
              " 'used',\n",
              " '3',\n",
              " 'lot',\n",
              " \"'m\",\n",
              " 'money',\n",
              " 'put',\n",
              " 'games',\n",
              " 'support',\n",
              " 'program',\n",
              " 'half',\n",
              " 'report',\n",
              " 'family',\n",
              " 'months',\n",
              " 'number',\n",
              " 'officials',\n",
              " 'am',\n",
              " 'former',\n",
              " 'own',\n",
              " 'man',\n",
              " 'Saturday',\n",
              " 'too',\n",
              " 'better',\n",
              " 'days',\n",
              " 'came',\n",
              " 'lead',\n",
              " 'life',\n",
              " 'American',\n",
              " '##-##',\n",
              " 'show',\n",
              " 'past',\n",
              " 'took',\n",
              " 'added',\n",
              " 'expected',\n",
              " 'called',\n",
              " 'great',\n",
              " 'State',\n",
              " 'services',\n",
              " 'children',\n",
              " 'hit',\n",
              " 'area',\n",
              " 'system',\n",
              " 'every',\n",
              " 'pm',\n",
              " 'big',\n",
              " 'service',\n",
              " 'few',\n",
              " 'per',\n",
              " 'members',\n",
              " 'Sunday',\n",
              " 'early',\n",
              " 'point',\n",
              " 'start',\n",
              " 'companies',\n",
              " 'little',\n",
              " '&',\n",
              " 'case',\n",
              " 'ago',\n",
              " 'local',\n",
              " 'according',\n",
              " 'never',\n",
              " '5',\n",
              " 'without',\n",
              " 'sales',\n",
              " 'until',\n",
              " 'went',\n",
              " 'players',\n",
              " '##th',\n",
              " 'New_York',\n",
              " 'won',\n",
              " 'financial',\n",
              " 'news',\n",
              " '4',\n",
              " 'When',\n",
              " 'share',\n",
              " 'several',\n",
              " 'free',\n",
              " 'away',\n",
              " '##.##',\n",
              " 'already',\n",
              " 'On',\n",
              " 'industry',\n",
              " \"'ll\",\n",
              " 'call',\n",
              " 'With',\n",
              " 'students',\n",
              " 'line',\n",
              " 'available',\n",
              " 'County',\n",
              " 'making',\n",
              " 'held',\n",
              " 'final',\n",
              " '#:##',\n",
              " 'power',\n",
              " 'plan',\n",
              " 'might',\n",
              " 'least',\n",
              " 'look',\n",
              " 'forward',\n",
              " 'give',\n",
              " 'At',\n",
              " 'again',\n",
              " 'later',\n",
              " 'full',\n",
              " 'must',\n",
              " 'things',\n",
              " 'major',\n",
              " 'community',\n",
              " 'announced',\n",
              " 'open',\n",
              " 'record',\n",
              " 'reported',\n",
              " 'court',\n",
              " 'working',\n",
              " 'able',\n",
              " 'something',\n",
              " 'president',\n",
              " 'meeting',\n",
              " 'keep',\n",
              " 'March',\n",
              " 'future',\n",
              " 'far',\n",
              " 'deal',\n",
              " 'City',\n",
              " 'May',\n",
              " 'development',\n",
              " 'University',\n",
              " 'find',\n",
              " 'times',\n",
              " 'After',\n",
              " 'office',\n",
              " 'led',\n",
              " 'among',\n",
              " 'June',\n",
              " 'increase',\n",
              " 'China',\n",
              " 'John',\n",
              " 'whether',\n",
              " 'cost',\n",
              " 'security',\n",
              " 'job',\n",
              " 'less',\n",
              " 'head',\n",
              " 'seven',\n",
              " 'growth',\n",
              " 'lost',\n",
              " 'pay',\n",
              " 'looking',\n",
              " 'provide',\n",
              " '6',\n",
              " 'To',\n",
              " 'plans',\n",
              " 'products',\n",
              " 'car',\n",
              " 'recent',\n",
              " 'hard',\n",
              " 'always',\n",
              " 'include',\n",
              " 'women',\n",
              " 'across',\n",
              " 'tax',\n",
              " 'water',\n",
              " 'April',\n",
              " 'continue',\n",
              " 'important',\n",
              " 'different',\n",
              " 'close',\n",
              " '7',\n",
              " 'One',\n",
              " 'late',\n",
              " 'decision',\n",
              " 'current',\n",
              " 'law',\n",
              " 'within',\n",
              " 'along',\n",
              " 'played',\n",
              " 'move',\n",
              " 'United_States',\n",
              " 'enough',\n",
              " 'become',\n",
              " 'side',\n",
              " 'national',\n",
              " 'Inc.',\n",
              " 'results',\n",
              " 'level',\n",
              " 'loss',\n",
              " 'economic',\n",
              " 'coach',\n",
              " 'near',\n",
              " 'getting',\n",
              " 'price',\n",
              " 'Department',\n",
              " 'event',\n",
              " 'fourth',\n",
              " 'change',\n",
              " 'All',\n",
              " 'small',\n",
              " 'board',\n",
              " 'National',\n",
              " 'So',\n",
              " 'goal',\n",
              " 'taken',\n",
              " 'field',\n",
              " 'prices',\n",
              " 'weeks',\n",
              " 'men',\n",
              " 'asked',\n",
              " 'eight',\n",
              " 'data',\n",
              " 'shot',\n",
              " 'New',\n",
              " 'started',\n",
              " 'July',\n",
              " 'director',\n",
              " 'President',\n",
              " 'party',\n",
              " 'federal',\n",
              " 'done',\n",
              " 'political',\n",
              " 'minutes',\n",
              " 'taking',\n",
              " 'Company',\n",
              " 'technology',\n",
              " 'project',\n",
              " 'center',\n",
              " 'leading',\n",
              " 'issue',\n",
              " 'though',\n",
              " 'having',\n",
              " 'period',\n",
              " 'likely',\n",
              " 'scored',\n",
              " '8',\n",
              " 'strong',\n",
              " 'series',\n",
              " 'military',\n",
              " 'seen',\n",
              " 'trying',\n",
              " 'What',\n",
              " 'coming',\n",
              " 'process',\n",
              " 'building',\n",
              " 'behind',\n",
              " 'performance',\n",
              " 'management',\n",
              " 'Iraq',\n",
              " 'saying',\n",
              " 'earlier',\n",
              " 'believe',\n",
              " 'oil',\n",
              " 'given',\n",
              " 'Police',\n",
              " 'customers',\n",
              " 'due',\n",
              " 'following',\n",
              " 'term',\n",
              " 'others',\n",
              " 'statement',\n",
              " 'international',\n",
              " 'economy',\n",
              " 'health',\n",
              " 'thing',\n",
              " 'Obama',\n",
              " 'return',\n",
              " 'killed',\n",
              " 'Washington',\n",
              " 'further',\n",
              " 'However',\n",
              " 'doing',\n",
              " 'face',\n",
              " 'low',\n",
              " 'higher',\n",
              " 'site',\n",
              " 'once',\n",
              " 'yet',\n",
              " 'hours',\n",
              " 'America',\n",
              " 'control',\n",
              " 'received',\n",
              " 'rate',\n",
              " 'career',\n",
              " 'Bush',\n",
              " 'teams',\n",
              " 'known',\n",
              " 'offer',\n",
              " 'race',\n",
              " 'ever',\n",
              " 'experience',\n",
              " 'playing',\n",
              " 'name',\n",
              " 'possible',\n",
              " 'countries',\n",
              " 'Mr.',\n",
              " 'average',\n",
              " 'together',\n",
              " 'using',\n",
              " '9',\n",
              " 'cut',\n",
              " 'While',\n",
              " 'total',\n",
              " 'round',\n",
              " 'young',\n",
              " 'nearly',\n",
              " 'shares',\n",
              " 'member',\n",
              " 'campaign',\n",
              " 'media',\n",
              " 'needs',\n",
              " 'why',\n",
              " 'house',\n",
              " 'issues',\n",
              " 'costs',\n",
              " 'fire',\n",
              " '##-#',\n",
              " 'victory',\n",
              " 'player',\n",
              " 'began',\n",
              " 'sure',\n",
              " 'story',\n",
              " 'per_cent',\n",
              " 'North',\n",
              " 'His',\n",
              " 'staff',\n",
              " 'order',\n",
              " 'war',\n",
              " 'large',\n",
              " 'interest',\n",
              " 'stock',\n",
              " 'food',\n",
              " 'research',\n",
              " 'key',\n",
              " 'India',\n",
              " 'South',\n",
              " 'morning',\n",
              " 'conference',\n",
              " 'senior',\n",
              " 'global',\n",
              " 'Center',\n",
              " 'death',\n",
              " 'person',\n",
              " 'thought',\n",
              " 'gave',\n",
              " 'feel',\n",
              " 'energy',\n",
              " 'history',\n",
              " 'recently',\n",
              " 'largest',\n",
              " 'No.',\n",
              " 'general',\n",
              " 'official',\n",
              " 'released',\n",
              " 'wanted',\n",
              " 'meet',\n",
              " 'short',\n",
              " 'outside',\n",
              " 'running',\n",
              " 'live',\n",
              " 'ball',\n",
              " 'online',\n",
              " 'real',\n",
              " 'position',\n",
              " 'fact',\n",
              " 'fell',\n",
              " 'nine',\n",
              " 'December',\n",
              " 'front',\n",
              " 'action',\n",
              " 'defense',\n",
              " 'problem',\n",
              " 'problems',\n",
              " 'Mr',\n",
              " 'nation',\n",
              " 'needed',\n",
              " 'special',\n",
              " 'January',\n",
              " 'almost',\n",
              " 'chance',\n",
              " \"'d\",\n",
              " 'result',\n",
              " 'West',\n",
              " 'September',\n",
              " 'reports',\n",
              " 'leader',\n",
              " 'investment',\n",
              " 'yesterday',\n",
              " 'Some',\n",
              " 'leaders',\n",
              " 'ahead',\n",
              " 'production',\n",
              " 'comes',\n",
              " 'No',\n",
              " 'runs',\n",
              " 'match',\n",
              " 'role',\n",
              " 'kind',\n",
              " 'try',\n",
              " 'ended',\n",
              " 'risk',\n",
              " 'areas',\n",
              " 'election',\n",
              " 'workers',\n",
              " 'visit',\n",
              " 'bring',\n",
              " 'road',\n",
              " 'music',\n",
              " 'study',\n",
              " 'makes',\n",
              " 'often',\n",
              " 'release',\n",
              " 'woman',\n",
              " 'vote',\n",
              " 'care',\n",
              " 'town',\n",
              " 'clear',\n",
              " 'comment',\n",
              " 'budget',\n",
              " 'potential',\n",
              " 'single',\n",
              " 'markets',\n",
              " 'policy',\n",
              " 'capital',\n",
              " 'saw',\n",
              " 'access',\n",
              " 'weekend',\n",
              " 'operations',\n",
              " 'whose',\n",
              " 'net',\n",
              " 'House',\n",
              " 'hand',\n",
              " 'increased',\n",
              " 'charges',\n",
              " 'winning',\n",
              " 'trade',\n",
              " 'These',\n",
              " 'income',\n",
              " 'value',\n",
              " 'involved',\n",
              " 'Bank',\n",
              " 'November',\n",
              " 'bill',\n",
              " 'compared',\n",
              " 'anything',\n",
              " 'manager',\n",
              " 'Texas',\n",
              " 'property',\n",
              " 'stop',\n",
              " 'annual',\n",
              " 'private',\n",
              " 'contract',\n",
              " 'died',\n",
              " 'Now',\n",
              " 'hope',\n",
              " 'product',\n",
              " 'fans',\n",
              " 'lower',\n",
              " 'demand',\n",
              " 'News',\n",
              " 'David',\n",
              " 'club',\n",
              " 'comments',\n",
              " 'film',\n",
              " 'yards',\n",
              " 'quality',\n",
              " 'currently',\n",
              " 'events',\n",
              " 'addition',\n",
              " 'couple',\n",
              " 'schools',\n",
              " 'attack',\n",
              " 'region',\n",
              " 'latest',\n",
              " 'opportunity',\n",
              " 'worked',\n",
              " 'course',\n",
              " 'bad',\n",
              " 'fall',\n",
              " 'Group',\n",
              " 'October',\n",
              " 'jobs',\n",
              " 'list',\n",
              " 'let',\n",
              " 'however',\n",
              " 'chief',\n",
              " 'summer',\n",
              " 'programs',\n",
              " 'According',\n",
              " 'revenue',\n",
              " 'Our',\n",
              " 'rose',\n",
              " 'previous',\n",
              " 'TV',\n",
              " 'football',\n",
              " 'biggest',\n",
              " 'employees',\n",
              " 'changes',\n",
              " 'residents',\n",
              " 'means',\n",
              " 'agreement',\n",
              " 'includes',\n",
              " 'post',\n",
              " 'Canada',\n",
              " 'probably',\n",
              " 'related',\n",
              " 'training',\n",
              " 'allowed',\n",
              " 'class',\n",
              " 'bit',\n",
              " 'video',\n",
              " 'Michael',\n",
              " 'An',\n",
              " 'sent',\n",
              " 'education',\n",
              " 'states',\n",
              " 'straight',\n",
              " 'love',\n",
              " 'beat',\n",
              " 'hold',\n",
              " 'turn',\n",
              " 'finished',\n",
              " 'network',\n",
              " 'Smith',\n",
              " 'buy',\n",
              " 'foreign',\n",
              " 'especially',\n",
              " 'groups',\n",
              " 'wants',\n",
              " 'title',\n",
              " 'included',\n",
              " 'turned',\n",
              " 'bank',\n",
              " 'Florida',\n",
              " 'efforts',\n",
              " 'personal',\n",
              " 'businesses',\n",
              " 'August',\n",
              " 'California',\n",
              " 'situation',\n",
              " 'district',\n",
              " 'allow',\n",
              " 'helped',\n",
              " 'body',\n",
              " 'nothing',\n",
              " 'soon',\n",
              " 'safety',\n",
              " 'officer',\n",
              " 'cents',\n",
              " 'Europe',\n",
              " 'St.',\n",
              " 'additional',\n",
              " 'spokesman',\n",
              " 'February',\n",
              " 'wife',\n",
              " 'showed',\n",
              " 'leave',\n",
              " 'investors',\n",
              " 'parents',\n",
              " 'medical',\n",
              " 'spending',\n",
              " 'non',\n",
              " 'London',\n",
              " 'Council',\n",
              " 'matter',\n",
              " 'spent',\n",
              " 'child',\n",
              " 'World',\n",
              " 'effort',\n",
              " 'opening',\n",
              " 'either',\n",
              " 'range',\n",
              " 'question',\n",
              " 'European',\n",
              " 'goals',\n",
              " 'administration',\n",
              " 'friends',\n",
              " 'himself',\n",
              " 'shows',\n",
              " 'difficult',\n",
              " 'kids',\n",
              " 'paid',\n",
              " 'create',\n",
              " 'cash',\n",
              " 'age',\n",
              " 'league',\n",
              " 'form',\n",
              " 'impact',\n",
              " 'drive',\n",
              " 'someone',\n",
              " 'became',\n",
              " 'stay',\n",
              " 'fight',\n",
              " 'significant',\n",
              " 'firm',\n",
              " 'Senate',\n",
              " 'hospital',\n",
              " 'charged',\n",
              " 'operating',\n",
              " 'main',\n",
              " 'book',\n",
              " 'success',\n",
              " 'son',\n",
              " 'trading',\n",
              " '###-####',\n",
              " 'focus',\n",
              " 'room',\n",
              " 'continued',\n",
              " 'Congress',\n",
              " 'everything',\n",
              " 'Park',\n",
              " 'agency',\n",
              " 'brought',\n",
              " 'talk',\n",
              " 'break',\n",
              " 'air',\n",
              " 'software',\n",
              " 'decided',\n",
              " 'Do',\n",
              " 'ready',\n",
              " 'arrested',\n",
              " 'track',\n",
              " 'provides',\n",
              " 'mother',\n",
              " 'base',\n",
              " 'trial',\n",
              " 'phone',\n",
              " 'My',\n",
              " 'build',\n",
              " 'conditions',\n",
              " 'rest',\n",
              " 'Johnson',\n",
              " 'terms',\n",
              " 'expect',\n",
              " 'England',\n",
              " 'Israel',\n",
              " 'despite',\n",
              " 'closed',\n",
              " 'starting',\n",
              " 'provided',\n",
              " 'pressure',\n",
              " 'lives',\n",
              " 'step',\n",
              " 'remain',\n",
              " 'similar',\n",
              " 'charge',\n",
              " 'date',\n",
              " 'whole',\n",
              " 'land',\n",
              " 'growing',\n",
              " 'James',\n",
              " 'Internet',\n",
              " 'projects',\n",
              " 'British',\n",
              " 'cases',\n",
              " 'ground',\n",
              " 'legal',\n",
              " 'International',\n",
              " 'agreed',\n",
              " 'tell',\n",
              " 'test',\n",
              " 'everyone',\n",
              " 'pretty',\n",
              " 'authorities',\n",
              " 'Two',\n",
              " 'above',\n",
              " 'moved',\n",
              " 'profit',\n",
              " 'throughout',\n",
              " 'inside',\n",
              " 'ability',\n",
              " 'overall',\n",
              " 'pass',\n",
              " 'officers',\n",
              " 'rather',\n",
              " 'Australia',\n",
              " 'actually',\n",
              " 'county',\n",
              " 'amount',\n",
              " 'scheduled',\n",
              " 'themselves',\n",
              " 'organization',\n",
              " 'giving',\n",
              " 'credit',\n",
              " 'father',\n",
              " 'drug',\n",
              " 'investigation',\n",
              " 'families',\n",
              " 'Republican',\n",
              " 'funds',\n",
              " 'patients',\n",
              " 'takes',\n",
              " 'systems',\n",
              " 'Japan',\n",
              " 'complete',\n",
              " 'sold',\n",
              " 'practice',\n",
              " 'calls',\n",
              " '•',\n",
              " 'UK',\n",
              " 'force',\n",
              " 'student',\n",
              " 'idea',\n",
              " 'reached',\n",
              " 'reason',\n",
              " 'levels',\n",
              " 'space',\n",
              " 'competition',\n",
              " 'forces',\n",
              " 'sector',\n",
              " 'Last',\n",
              " 'tried',\n",
              " 'common',\n",
              " 'homes',\n",
              " 'stage',\n",
              " 'department',\n",
              " 'named',\n",
              " 'earnings',\n",
              " 'offers',\n",
              " 'star',\n",
              " 'certain',\n",
              " 'double',\n",
              " 'longer',\n",
              " 'followed',\n",
              " 'cause',\n",
              " 'Association',\n",
              " 'signed',\n",
              " 'committee',\n",
              " 'hour',\n",
              " 'college',\n",
              " 'Pakistan',\n",
              " 'users',\n",
              " 'Iran',\n",
              " 'sign',\n",
              " 'living',\n",
              " 'failed',\n",
              " 'reach',\n",
              " 'quickly',\n",
              " 'receive',\n",
              " 'debt',\n",
              " 'sale',\n",
              " 'Board',\n",
              " 'Americans',\n",
              " 'Road',\n",
              " 'Brown',\n",
              " 'insurance',\n",
              " '##:##',\n",
              " 'anyone',\n",
              " 'tournament',\n",
              " 'More',\n",
              " 'gas',\n",
              " 'talks',\n",
              " 'serious',\n",
              " 'required',\n",
              " 'sell',\n",
              " 'construction',\n",
              " 'evidence',\n",
              " 'remains',\n",
              " 'black',\n",
              " 'below',\n",
              " 'improve',\n",
              " 'crisis',\n",
              " 'address',\n",
              " 'questions',\n",
              " 'easy',\n",
              " 'begin',\n",
              " 'view',\n",
              " 'School',\n",
              " 'heard',\n",
              " 'executive',\n",
              " 'raised',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'</s>': 0,\n",
              " 'in': 1,\n",
              " 'for': 2,\n",
              " 'that': 3,\n",
              " 'is': 4,\n",
              " 'on': 5,\n",
              " '##': 6,\n",
              " 'The': 7,\n",
              " 'with': 8,\n",
              " 'said': 9,\n",
              " 'was': 10,\n",
              " 'the': 11,\n",
              " 'at': 12,\n",
              " 'not': 13,\n",
              " 'as': 14,\n",
              " 'it': 15,\n",
              " 'be': 16,\n",
              " 'from': 17,\n",
              " 'by': 18,\n",
              " 'are': 19,\n",
              " 'I': 20,\n",
              " 'have': 21,\n",
              " 'he': 22,\n",
              " 'will': 23,\n",
              " 'has': 24,\n",
              " '####': 25,\n",
              " 'his': 26,\n",
              " 'an': 27,\n",
              " 'this': 28,\n",
              " 'or': 29,\n",
              " 'their': 30,\n",
              " 'who': 31,\n",
              " 'they': 32,\n",
              " 'but': 33,\n",
              " '$': 34,\n",
              " 'had': 35,\n",
              " 'year': 36,\n",
              " 'were': 37,\n",
              " 'we': 38,\n",
              " 'more': 39,\n",
              " '###': 40,\n",
              " 'up': 41,\n",
              " 'been': 42,\n",
              " 'you': 43,\n",
              " 'its': 44,\n",
              " 'one': 45,\n",
              " 'about': 46,\n",
              " 'would': 47,\n",
              " 'which': 48,\n",
              " 'out': 49,\n",
              " 'can': 50,\n",
              " 'It': 51,\n",
              " 'all': 52,\n",
              " 'also': 53,\n",
              " 'two': 54,\n",
              " 'after': 55,\n",
              " 'first': 56,\n",
              " 'He': 57,\n",
              " 'do': 58,\n",
              " 'time': 59,\n",
              " 'than': 60,\n",
              " 'when': 61,\n",
              " 'We': 62,\n",
              " 'over': 63,\n",
              " 'last': 64,\n",
              " 'new': 65,\n",
              " 'other': 66,\n",
              " 'her': 67,\n",
              " 'people': 68,\n",
              " 'into': 69,\n",
              " 'In': 70,\n",
              " 'our': 71,\n",
              " 'there': 72,\n",
              " 'A': 73,\n",
              " 'she': 74,\n",
              " 'could': 75,\n",
              " 'just': 76,\n",
              " 'years': 77,\n",
              " 'some': 78,\n",
              " 'U.S.': 79,\n",
              " 'three': 80,\n",
              " 'million': 81,\n",
              " 'them': 82,\n",
              " 'what': 83,\n",
              " 'But': 84,\n",
              " 'so': 85,\n",
              " 'no': 86,\n",
              " 'like': 87,\n",
              " 'if': 88,\n",
              " 'only': 89,\n",
              " 'percent': 90,\n",
              " 'get': 91,\n",
              " 'did': 92,\n",
              " 'him': 93,\n",
              " 'game': 94,\n",
              " 'back': 95,\n",
              " 'because': 96,\n",
              " 'now': 97,\n",
              " '#.#': 98,\n",
              " 'before': 99,\n",
              " 'company': 100,\n",
              " 'any': 101,\n",
              " 'team': 102,\n",
              " 'against': 103,\n",
              " 'off': 104,\n",
              " 'This': 105,\n",
              " 'most': 106,\n",
              " 'made': 107,\n",
              " 'through': 108,\n",
              " 'make': 109,\n",
              " 'second': 110,\n",
              " 'state': 111,\n",
              " 'well': 112,\n",
              " 'day': 113,\n",
              " 'season': 114,\n",
              " 'says': 115,\n",
              " 'week': 116,\n",
              " 'where': 117,\n",
              " 'while': 118,\n",
              " 'down': 119,\n",
              " 'being': 120,\n",
              " 'government': 121,\n",
              " 'your': 122,\n",
              " '#-#': 123,\n",
              " 'home': 124,\n",
              " 'going': 125,\n",
              " 'my': 126,\n",
              " 'good': 127,\n",
              " 'They': 128,\n",
              " \"'re\": 129,\n",
              " 'should': 130,\n",
              " 'many': 131,\n",
              " 'way': 132,\n",
              " 'those': 133,\n",
              " 'four': 134,\n",
              " 'during': 135,\n",
              " 'such': 136,\n",
              " 'may': 137,\n",
              " 'very': 138,\n",
              " 'how': 139,\n",
              " 'since': 140,\n",
              " 'work': 141,\n",
              " 'take': 142,\n",
              " 'including': 143,\n",
              " 'high': 144,\n",
              " 'then': 145,\n",
              " '%': 146,\n",
              " 'next': 147,\n",
              " '#,###': 148,\n",
              " 'By': 149,\n",
              " 'much': 150,\n",
              " 'still': 151,\n",
              " 'go': 152,\n",
              " 'think': 153,\n",
              " 'old': 154,\n",
              " 'even': 155,\n",
              " '#.##': 156,\n",
              " 'world': 157,\n",
              " 'see': 158,\n",
              " 'say': 159,\n",
              " 'business': 160,\n",
              " 'five': 161,\n",
              " 'told': 162,\n",
              " 'under': 163,\n",
              " 'us': 164,\n",
              " '1': 165,\n",
              " 'these': 166,\n",
              " 'If': 167,\n",
              " 'right': 168,\n",
              " 'And': 169,\n",
              " 'me': 170,\n",
              " 'between': 171,\n",
              " 'play': 172,\n",
              " 'help': 173,\n",
              " '##,###': 174,\n",
              " 'market': 175,\n",
              " 'That': 176,\n",
              " 'know': 177,\n",
              " 'end': 178,\n",
              " 'AP': 179,\n",
              " 'long': 180,\n",
              " 'information': 181,\n",
              " 'points': 182,\n",
              " 'does': 183,\n",
              " 'both': 184,\n",
              " 'There': 185,\n",
              " 'part': 186,\n",
              " 'around': 187,\n",
              " 'police': 188,\n",
              " 'want': 189,\n",
              " \"'ve\": 190,\n",
              " 'based': 191,\n",
              " 'For': 192,\n",
              " 'got': 193,\n",
              " 'third': 194,\n",
              " 'school': 195,\n",
              " 'left': 196,\n",
              " 'another': 197,\n",
              " 'country': 198,\n",
              " 'need': 199,\n",
              " '2': 200,\n",
              " 'best': 201,\n",
              " 'win': 202,\n",
              " 'quarter': 203,\n",
              " 'use': 204,\n",
              " 'today': 205,\n",
              " '##.#': 206,\n",
              " 'same': 207,\n",
              " 'public': 208,\n",
              " 'run': 209,\n",
              " 'Friday': 210,\n",
              " 'set': 211,\n",
              " 'month': 212,\n",
              " 'top': 213,\n",
              " 'billion': 214,\n",
              " 'Tuesday': 215,\n",
              " 'come': 216,\n",
              " 'Monday': 217,\n",
              " 'She': 218,\n",
              " 'city': 219,\n",
              " 'place': 220,\n",
              " 'night': 221,\n",
              " 'six': 222,\n",
              " 'each': 223,\n",
              " 'Thursday': 224,\n",
              " '###,###': 225,\n",
              " 'Wednesday': 226,\n",
              " 'here': 227,\n",
              " 'You': 228,\n",
              " 'group': 229,\n",
              " 'really': 230,\n",
              " 'found': 231,\n",
              " 'As': 232,\n",
              " 'used': 233,\n",
              " '3': 234,\n",
              " 'lot': 235,\n",
              " \"'m\": 236,\n",
              " 'money': 237,\n",
              " 'put': 238,\n",
              " 'games': 239,\n",
              " 'support': 240,\n",
              " 'program': 241,\n",
              " 'half': 242,\n",
              " 'report': 243,\n",
              " 'family': 244,\n",
              " 'months': 245,\n",
              " 'number': 246,\n",
              " 'officials': 247,\n",
              " 'am': 248,\n",
              " 'former': 249,\n",
              " 'own': 250,\n",
              " 'man': 251,\n",
              " 'Saturday': 252,\n",
              " 'too': 253,\n",
              " 'better': 254,\n",
              " 'days': 255,\n",
              " 'came': 256,\n",
              " 'lead': 257,\n",
              " 'life': 258,\n",
              " 'American': 259,\n",
              " '##-##': 260,\n",
              " 'show': 261,\n",
              " 'past': 262,\n",
              " 'took': 263,\n",
              " 'added': 264,\n",
              " 'expected': 265,\n",
              " 'called': 266,\n",
              " 'great': 267,\n",
              " 'State': 268,\n",
              " 'services': 269,\n",
              " 'children': 270,\n",
              " 'hit': 271,\n",
              " 'area': 272,\n",
              " 'system': 273,\n",
              " 'every': 274,\n",
              " 'pm': 275,\n",
              " 'big': 276,\n",
              " 'service': 277,\n",
              " 'few': 278,\n",
              " 'per': 279,\n",
              " 'members': 280,\n",
              " 'Sunday': 281,\n",
              " 'early': 282,\n",
              " 'point': 283,\n",
              " 'start': 284,\n",
              " 'companies': 285,\n",
              " 'little': 286,\n",
              " '&': 287,\n",
              " 'case': 288,\n",
              " 'ago': 289,\n",
              " 'local': 290,\n",
              " 'according': 291,\n",
              " 'never': 292,\n",
              " '5': 293,\n",
              " 'without': 294,\n",
              " 'sales': 295,\n",
              " 'until': 296,\n",
              " 'went': 297,\n",
              " 'players': 298,\n",
              " '##th': 299,\n",
              " 'New_York': 300,\n",
              " 'won': 301,\n",
              " 'financial': 302,\n",
              " 'news': 303,\n",
              " '4': 304,\n",
              " 'When': 305,\n",
              " 'share': 306,\n",
              " 'several': 307,\n",
              " 'free': 308,\n",
              " 'away': 309,\n",
              " '##.##': 310,\n",
              " 'already': 311,\n",
              " 'On': 312,\n",
              " 'industry': 313,\n",
              " \"'ll\": 314,\n",
              " 'call': 315,\n",
              " 'With': 316,\n",
              " 'students': 317,\n",
              " 'line': 318,\n",
              " 'available': 319,\n",
              " 'County': 320,\n",
              " 'making': 321,\n",
              " 'held': 322,\n",
              " 'final': 323,\n",
              " '#:##': 324,\n",
              " 'power': 325,\n",
              " 'plan': 326,\n",
              " 'might': 327,\n",
              " 'least': 328,\n",
              " 'look': 329,\n",
              " 'forward': 330,\n",
              " 'give': 331,\n",
              " 'At': 332,\n",
              " 'again': 333,\n",
              " 'later': 334,\n",
              " 'full': 335,\n",
              " 'must': 336,\n",
              " 'things': 337,\n",
              " 'major': 338,\n",
              " 'community': 339,\n",
              " 'announced': 340,\n",
              " 'open': 341,\n",
              " 'record': 342,\n",
              " 'reported': 343,\n",
              " 'court': 344,\n",
              " 'working': 345,\n",
              " 'able': 346,\n",
              " 'something': 347,\n",
              " 'president': 348,\n",
              " 'meeting': 349,\n",
              " 'keep': 350,\n",
              " 'March': 351,\n",
              " 'future': 352,\n",
              " 'far': 353,\n",
              " 'deal': 354,\n",
              " 'City': 355,\n",
              " 'May': 356,\n",
              " 'development': 357,\n",
              " 'University': 358,\n",
              " 'find': 359,\n",
              " 'times': 360,\n",
              " 'After': 361,\n",
              " 'office': 362,\n",
              " 'led': 363,\n",
              " 'among': 364,\n",
              " 'June': 365,\n",
              " 'increase': 366,\n",
              " 'China': 367,\n",
              " 'John': 368,\n",
              " 'whether': 369,\n",
              " 'cost': 370,\n",
              " 'security': 371,\n",
              " 'job': 372,\n",
              " 'less': 373,\n",
              " 'head': 374,\n",
              " 'seven': 375,\n",
              " 'growth': 376,\n",
              " 'lost': 377,\n",
              " 'pay': 378,\n",
              " 'looking': 379,\n",
              " 'provide': 380,\n",
              " '6': 381,\n",
              " 'To': 382,\n",
              " 'plans': 383,\n",
              " 'products': 384,\n",
              " 'car': 385,\n",
              " 'recent': 386,\n",
              " 'hard': 387,\n",
              " 'always': 388,\n",
              " 'include': 389,\n",
              " 'women': 390,\n",
              " 'across': 391,\n",
              " 'tax': 392,\n",
              " 'water': 393,\n",
              " 'April': 394,\n",
              " 'continue': 395,\n",
              " 'important': 396,\n",
              " 'different': 397,\n",
              " 'close': 398,\n",
              " '7': 399,\n",
              " 'One': 400,\n",
              " 'late': 401,\n",
              " 'decision': 402,\n",
              " 'current': 403,\n",
              " 'law': 404,\n",
              " 'within': 405,\n",
              " 'along': 406,\n",
              " 'played': 407,\n",
              " 'move': 408,\n",
              " 'United_States': 409,\n",
              " 'enough': 410,\n",
              " 'become': 411,\n",
              " 'side': 412,\n",
              " 'national': 413,\n",
              " 'Inc.': 414,\n",
              " 'results': 415,\n",
              " 'level': 416,\n",
              " 'loss': 417,\n",
              " 'economic': 418,\n",
              " 'coach': 419,\n",
              " 'near': 420,\n",
              " 'getting': 421,\n",
              " 'price': 422,\n",
              " 'Department': 423,\n",
              " 'event': 424,\n",
              " 'fourth': 425,\n",
              " 'change': 426,\n",
              " 'All': 427,\n",
              " 'small': 428,\n",
              " 'board': 429,\n",
              " 'National': 430,\n",
              " 'So': 431,\n",
              " 'goal': 432,\n",
              " 'taken': 433,\n",
              " 'field': 434,\n",
              " 'prices': 435,\n",
              " 'weeks': 436,\n",
              " 'men': 437,\n",
              " 'asked': 438,\n",
              " 'eight': 439,\n",
              " 'data': 440,\n",
              " 'shot': 441,\n",
              " 'New': 442,\n",
              " 'started': 443,\n",
              " 'July': 444,\n",
              " 'director': 445,\n",
              " 'President': 446,\n",
              " 'party': 447,\n",
              " 'federal': 448,\n",
              " 'done': 449,\n",
              " 'political': 450,\n",
              " 'minutes': 451,\n",
              " 'taking': 452,\n",
              " 'Company': 453,\n",
              " 'technology': 454,\n",
              " 'project': 455,\n",
              " 'center': 456,\n",
              " 'leading': 457,\n",
              " 'issue': 458,\n",
              " 'though': 459,\n",
              " 'having': 460,\n",
              " 'period': 461,\n",
              " 'likely': 462,\n",
              " 'scored': 463,\n",
              " '8': 464,\n",
              " 'strong': 465,\n",
              " 'series': 466,\n",
              " 'military': 467,\n",
              " 'seen': 468,\n",
              " 'trying': 469,\n",
              " 'What': 470,\n",
              " 'coming': 471,\n",
              " 'process': 472,\n",
              " 'building': 473,\n",
              " 'behind': 474,\n",
              " 'performance': 475,\n",
              " 'management': 476,\n",
              " 'Iraq': 477,\n",
              " 'saying': 478,\n",
              " 'earlier': 479,\n",
              " 'believe': 480,\n",
              " 'oil': 481,\n",
              " 'given': 482,\n",
              " 'Police': 483,\n",
              " 'customers': 484,\n",
              " 'due': 485,\n",
              " 'following': 486,\n",
              " 'term': 487,\n",
              " 'others': 488,\n",
              " 'statement': 489,\n",
              " 'international': 490,\n",
              " 'economy': 491,\n",
              " 'health': 492,\n",
              " 'thing': 493,\n",
              " 'Obama': 494,\n",
              " 'return': 495,\n",
              " 'killed': 496,\n",
              " 'Washington': 497,\n",
              " 'further': 498,\n",
              " 'However': 499,\n",
              " 'doing': 500,\n",
              " 'face': 501,\n",
              " 'low': 502,\n",
              " 'higher': 503,\n",
              " 'site': 504,\n",
              " 'once': 505,\n",
              " 'yet': 506,\n",
              " 'hours': 507,\n",
              " 'America': 508,\n",
              " 'control': 509,\n",
              " 'received': 510,\n",
              " 'rate': 511,\n",
              " 'career': 512,\n",
              " 'Bush': 513,\n",
              " 'teams': 514,\n",
              " 'known': 515,\n",
              " 'offer': 516,\n",
              " 'race': 517,\n",
              " 'ever': 518,\n",
              " 'experience': 519,\n",
              " 'playing': 520,\n",
              " 'name': 521,\n",
              " 'possible': 522,\n",
              " 'countries': 523,\n",
              " 'Mr.': 524,\n",
              " 'average': 525,\n",
              " 'together': 526,\n",
              " 'using': 527,\n",
              " '9': 528,\n",
              " 'cut': 529,\n",
              " 'While': 530,\n",
              " 'total': 531,\n",
              " 'round': 532,\n",
              " 'young': 533,\n",
              " 'nearly': 534,\n",
              " 'shares': 535,\n",
              " 'member': 536,\n",
              " 'campaign': 537,\n",
              " 'media': 538,\n",
              " 'needs': 539,\n",
              " 'why': 540,\n",
              " 'house': 541,\n",
              " 'issues': 542,\n",
              " 'costs': 543,\n",
              " 'fire': 544,\n",
              " '##-#': 545,\n",
              " 'victory': 546,\n",
              " 'player': 547,\n",
              " 'began': 548,\n",
              " 'sure': 549,\n",
              " 'story': 550,\n",
              " 'per_cent': 551,\n",
              " 'North': 552,\n",
              " 'His': 553,\n",
              " 'staff': 554,\n",
              " 'order': 555,\n",
              " 'war': 556,\n",
              " 'large': 557,\n",
              " 'interest': 558,\n",
              " 'stock': 559,\n",
              " 'food': 560,\n",
              " 'research': 561,\n",
              " 'key': 562,\n",
              " 'India': 563,\n",
              " 'South': 564,\n",
              " 'morning': 565,\n",
              " 'conference': 566,\n",
              " 'senior': 567,\n",
              " 'global': 568,\n",
              " 'Center': 569,\n",
              " 'death': 570,\n",
              " 'person': 571,\n",
              " 'thought': 572,\n",
              " 'gave': 573,\n",
              " 'feel': 574,\n",
              " 'energy': 575,\n",
              " 'history': 576,\n",
              " 'recently': 577,\n",
              " 'largest': 578,\n",
              " 'No.': 579,\n",
              " 'general': 580,\n",
              " 'official': 581,\n",
              " 'released': 582,\n",
              " 'wanted': 583,\n",
              " 'meet': 584,\n",
              " 'short': 585,\n",
              " 'outside': 586,\n",
              " 'running': 587,\n",
              " 'live': 588,\n",
              " 'ball': 589,\n",
              " 'online': 590,\n",
              " 'real': 591,\n",
              " 'position': 592,\n",
              " 'fact': 593,\n",
              " 'fell': 594,\n",
              " 'nine': 595,\n",
              " 'December': 596,\n",
              " 'front': 597,\n",
              " 'action': 598,\n",
              " 'defense': 599,\n",
              " 'problem': 600,\n",
              " 'problems': 601,\n",
              " 'Mr': 602,\n",
              " 'nation': 603,\n",
              " 'needed': 604,\n",
              " 'special': 605,\n",
              " 'January': 606,\n",
              " 'almost': 607,\n",
              " 'chance': 608,\n",
              " \"'d\": 609,\n",
              " 'result': 610,\n",
              " 'West': 611,\n",
              " 'September': 612,\n",
              " 'reports': 613,\n",
              " 'leader': 614,\n",
              " 'investment': 615,\n",
              " 'yesterday': 616,\n",
              " 'Some': 617,\n",
              " 'leaders': 618,\n",
              " 'ahead': 619,\n",
              " 'production': 620,\n",
              " 'comes': 621,\n",
              " 'No': 622,\n",
              " 'runs': 623,\n",
              " 'match': 624,\n",
              " 'role': 625,\n",
              " 'kind': 626,\n",
              " 'try': 627,\n",
              " 'ended': 628,\n",
              " 'risk': 629,\n",
              " 'areas': 630,\n",
              " 'election': 631,\n",
              " 'workers': 632,\n",
              " 'visit': 633,\n",
              " 'bring': 634,\n",
              " 'road': 635,\n",
              " 'music': 636,\n",
              " 'study': 637,\n",
              " 'makes': 638,\n",
              " 'often': 639,\n",
              " 'release': 640,\n",
              " 'woman': 641,\n",
              " 'vote': 642,\n",
              " 'care': 643,\n",
              " 'town': 644,\n",
              " 'clear': 645,\n",
              " 'comment': 646,\n",
              " 'budget': 647,\n",
              " 'potential': 648,\n",
              " 'single': 649,\n",
              " 'markets': 650,\n",
              " 'policy': 651,\n",
              " 'capital': 652,\n",
              " 'saw': 653,\n",
              " 'access': 654,\n",
              " 'weekend': 655,\n",
              " 'operations': 656,\n",
              " 'whose': 657,\n",
              " 'net': 658,\n",
              " 'House': 659,\n",
              " 'hand': 660,\n",
              " 'increased': 661,\n",
              " 'charges': 662,\n",
              " 'winning': 663,\n",
              " 'trade': 664,\n",
              " 'These': 665,\n",
              " 'income': 666,\n",
              " 'value': 667,\n",
              " 'involved': 668,\n",
              " 'Bank': 669,\n",
              " 'November': 670,\n",
              " 'bill': 671,\n",
              " 'compared': 672,\n",
              " 'anything': 673,\n",
              " 'manager': 674,\n",
              " 'Texas': 675,\n",
              " 'property': 676,\n",
              " 'stop': 677,\n",
              " 'annual': 678,\n",
              " 'private': 679,\n",
              " 'contract': 680,\n",
              " 'died': 681,\n",
              " 'Now': 682,\n",
              " 'hope': 683,\n",
              " 'product': 684,\n",
              " 'fans': 685,\n",
              " 'lower': 686,\n",
              " 'demand': 687,\n",
              " 'News': 688,\n",
              " 'David': 689,\n",
              " 'club': 690,\n",
              " 'comments': 691,\n",
              " 'film': 692,\n",
              " 'yards': 693,\n",
              " 'quality': 694,\n",
              " 'currently': 695,\n",
              " 'events': 696,\n",
              " 'addition': 697,\n",
              " 'couple': 698,\n",
              " 'schools': 699,\n",
              " 'attack': 700,\n",
              " 'region': 701,\n",
              " 'latest': 702,\n",
              " 'opportunity': 703,\n",
              " 'worked': 704,\n",
              " 'course': 705,\n",
              " 'bad': 706,\n",
              " 'fall': 707,\n",
              " 'Group': 708,\n",
              " 'October': 709,\n",
              " 'jobs': 710,\n",
              " 'list': 711,\n",
              " 'let': 712,\n",
              " 'however': 713,\n",
              " 'chief': 714,\n",
              " 'summer': 715,\n",
              " 'programs': 716,\n",
              " 'According': 717,\n",
              " 'revenue': 718,\n",
              " 'Our': 719,\n",
              " 'rose': 720,\n",
              " 'previous': 721,\n",
              " 'TV': 722,\n",
              " 'football': 723,\n",
              " 'biggest': 724,\n",
              " 'employees': 725,\n",
              " 'changes': 726,\n",
              " 'residents': 727,\n",
              " 'means': 728,\n",
              " 'agreement': 729,\n",
              " 'includes': 730,\n",
              " 'post': 731,\n",
              " 'Canada': 732,\n",
              " 'probably': 733,\n",
              " 'related': 734,\n",
              " 'training': 735,\n",
              " 'allowed': 736,\n",
              " 'class': 737,\n",
              " 'bit': 738,\n",
              " 'video': 739,\n",
              " 'Michael': 740,\n",
              " 'An': 741,\n",
              " 'sent': 742,\n",
              " 'education': 743,\n",
              " 'states': 744,\n",
              " 'straight': 745,\n",
              " 'love': 746,\n",
              " 'beat': 747,\n",
              " 'hold': 748,\n",
              " 'turn': 749,\n",
              " 'finished': 750,\n",
              " 'network': 751,\n",
              " 'Smith': 752,\n",
              " 'buy': 753,\n",
              " 'foreign': 754,\n",
              " 'especially': 755,\n",
              " 'groups': 756,\n",
              " 'wants': 757,\n",
              " 'title': 758,\n",
              " 'included': 759,\n",
              " 'turned': 760,\n",
              " 'bank': 761,\n",
              " 'Florida': 762,\n",
              " 'efforts': 763,\n",
              " 'personal': 764,\n",
              " 'businesses': 765,\n",
              " 'August': 766,\n",
              " 'California': 767,\n",
              " 'situation': 768,\n",
              " 'district': 769,\n",
              " 'allow': 770,\n",
              " 'helped': 771,\n",
              " 'body': 772,\n",
              " 'nothing': 773,\n",
              " 'soon': 774,\n",
              " 'safety': 775,\n",
              " 'officer': 776,\n",
              " 'cents': 777,\n",
              " 'Europe': 778,\n",
              " 'St.': 779,\n",
              " 'additional': 780,\n",
              " 'spokesman': 781,\n",
              " 'February': 782,\n",
              " 'wife': 783,\n",
              " 'showed': 784,\n",
              " 'leave': 785,\n",
              " 'investors': 786,\n",
              " 'parents': 787,\n",
              " 'medical': 788,\n",
              " 'spending': 789,\n",
              " 'non': 790,\n",
              " 'London': 791,\n",
              " 'Council': 792,\n",
              " 'matter': 793,\n",
              " 'spent': 794,\n",
              " 'child': 795,\n",
              " 'World': 796,\n",
              " 'effort': 797,\n",
              " 'opening': 798,\n",
              " 'either': 799,\n",
              " 'range': 800,\n",
              " 'question': 801,\n",
              " 'European': 802,\n",
              " 'goals': 803,\n",
              " 'administration': 804,\n",
              " 'friends': 805,\n",
              " 'himself': 806,\n",
              " 'shows': 807,\n",
              " 'difficult': 808,\n",
              " 'kids': 809,\n",
              " 'paid': 810,\n",
              " 'create': 811,\n",
              " 'cash': 812,\n",
              " 'age': 813,\n",
              " 'league': 814,\n",
              " 'form': 815,\n",
              " 'impact': 816,\n",
              " 'drive': 817,\n",
              " 'someone': 818,\n",
              " 'became': 819,\n",
              " 'stay': 820,\n",
              " 'fight': 821,\n",
              " 'significant': 822,\n",
              " 'firm': 823,\n",
              " 'Senate': 824,\n",
              " 'hospital': 825,\n",
              " 'charged': 826,\n",
              " 'operating': 827,\n",
              " 'main': 828,\n",
              " 'book': 829,\n",
              " 'success': 830,\n",
              " 'son': 831,\n",
              " 'trading': 832,\n",
              " '###-####': 833,\n",
              " 'focus': 834,\n",
              " 'room': 835,\n",
              " 'continued': 836,\n",
              " 'Congress': 837,\n",
              " 'everything': 838,\n",
              " 'Park': 839,\n",
              " 'agency': 840,\n",
              " 'brought': 841,\n",
              " 'talk': 842,\n",
              " 'break': 843,\n",
              " 'air': 844,\n",
              " 'software': 845,\n",
              " 'decided': 846,\n",
              " 'Do': 847,\n",
              " 'ready': 848,\n",
              " 'arrested': 849,\n",
              " 'track': 850,\n",
              " 'provides': 851,\n",
              " 'mother': 852,\n",
              " 'base': 853,\n",
              " 'trial': 854,\n",
              " 'phone': 855,\n",
              " 'My': 856,\n",
              " 'build': 857,\n",
              " 'conditions': 858,\n",
              " 'rest': 859,\n",
              " 'Johnson': 860,\n",
              " 'terms': 861,\n",
              " 'expect': 862,\n",
              " 'England': 863,\n",
              " 'Israel': 864,\n",
              " 'despite': 865,\n",
              " 'closed': 866,\n",
              " 'starting': 867,\n",
              " 'provided': 868,\n",
              " 'pressure': 869,\n",
              " 'lives': 870,\n",
              " 'step': 871,\n",
              " 'remain': 872,\n",
              " 'similar': 873,\n",
              " 'charge': 874,\n",
              " 'date': 875,\n",
              " 'whole': 876,\n",
              " 'land': 877,\n",
              " 'growing': 878,\n",
              " 'James': 879,\n",
              " 'Internet': 880,\n",
              " 'projects': 881,\n",
              " 'British': 882,\n",
              " 'cases': 883,\n",
              " 'ground': 884,\n",
              " 'legal': 885,\n",
              " 'International': 886,\n",
              " 'agreed': 887,\n",
              " 'tell': 888,\n",
              " 'test': 889,\n",
              " 'everyone': 890,\n",
              " 'pretty': 891,\n",
              " 'authorities': 892,\n",
              " 'Two': 893,\n",
              " 'above': 894,\n",
              " 'moved': 895,\n",
              " 'profit': 896,\n",
              " 'throughout': 897,\n",
              " 'inside': 898,\n",
              " 'ability': 899,\n",
              " 'overall': 900,\n",
              " 'pass': 901,\n",
              " 'officers': 902,\n",
              " 'rather': 903,\n",
              " 'Australia': 904,\n",
              " 'actually': 905,\n",
              " 'county': 906,\n",
              " 'amount': 907,\n",
              " 'scheduled': 908,\n",
              " 'themselves': 909,\n",
              " 'organization': 910,\n",
              " 'giving': 911,\n",
              " 'credit': 912,\n",
              " 'father': 913,\n",
              " 'drug': 914,\n",
              " 'investigation': 915,\n",
              " 'families': 916,\n",
              " 'Republican': 917,\n",
              " 'funds': 918,\n",
              " 'patients': 919,\n",
              " 'takes': 920,\n",
              " 'systems': 921,\n",
              " 'Japan': 922,\n",
              " 'complete': 923,\n",
              " 'sold': 924,\n",
              " 'practice': 925,\n",
              " 'calls': 926,\n",
              " '•': 927,\n",
              " 'UK': 928,\n",
              " 'force': 929,\n",
              " 'student': 930,\n",
              " 'idea': 931,\n",
              " 'reached': 932,\n",
              " 'reason': 933,\n",
              " 'levels': 934,\n",
              " 'space': 935,\n",
              " 'competition': 936,\n",
              " 'forces': 937,\n",
              " 'sector': 938,\n",
              " 'Last': 939,\n",
              " 'tried': 940,\n",
              " 'common': 941,\n",
              " 'homes': 942,\n",
              " 'stage': 943,\n",
              " 'department': 944,\n",
              " 'named': 945,\n",
              " 'earnings': 946,\n",
              " 'offers': 947,\n",
              " 'star': 948,\n",
              " 'certain': 949,\n",
              " 'double': 950,\n",
              " 'longer': 951,\n",
              " 'followed': 952,\n",
              " 'cause': 953,\n",
              " 'Association': 954,\n",
              " 'signed': 955,\n",
              " 'committee': 956,\n",
              " 'hour': 957,\n",
              " 'college': 958,\n",
              " 'Pakistan': 959,\n",
              " 'users': 960,\n",
              " 'Iran': 961,\n",
              " 'sign': 962,\n",
              " 'living': 963,\n",
              " 'failed': 964,\n",
              " 'reach': 965,\n",
              " 'quickly': 966,\n",
              " 'receive': 967,\n",
              " 'debt': 968,\n",
              " 'sale': 969,\n",
              " 'Board': 970,\n",
              " 'Americans': 971,\n",
              " 'Road': 972,\n",
              " 'Brown': 973,\n",
              " 'insurance': 974,\n",
              " '##:##': 975,\n",
              " 'anyone': 976,\n",
              " 'tournament': 977,\n",
              " 'More': 978,\n",
              " 'gas': 979,\n",
              " 'talks': 980,\n",
              " 'serious': 981,\n",
              " 'required': 982,\n",
              " 'sell': 983,\n",
              " 'construction': 984,\n",
              " 'evidence': 985,\n",
              " 'remains': 986,\n",
              " 'black': 987,\n",
              " 'below': 988,\n",
              " 'improve': 989,\n",
              " 'crisis': 990,\n",
              " 'address': 991,\n",
              " 'questions': 992,\n",
              " 'easy': 993,\n",
              " 'begin': 994,\n",
              " 'view': 995,\n",
              " 'School': 996,\n",
              " 'heard': 997,\n",
              " 'executive': 998,\n",
              " 'raised': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.1292e-03, -8.9645e-04,  3.1853e-04,  ..., -1.5640e-03,\n",
              "         -1.2302e-04, -8.6308e-05],\n",
              "        [ 7.0312e-02,  8.6914e-02,  8.7891e-02,  ..., -4.7607e-02,\n",
              "          1.4465e-02, -6.2500e-02],\n",
              "        [-1.1780e-02, -4.7363e-02,  4.4678e-02,  ...,  7.1289e-02,\n",
              "         -3.4912e-02,  2.4170e-02],\n",
              "        ...,\n",
              "        [-1.9653e-02, -9.0820e-02, -1.9409e-02,  ..., -1.6357e-02,\n",
              "         -1.3428e-02,  4.6631e-02],\n",
              "        [ 3.2715e-02, -3.2227e-02,  3.6133e-02,  ..., -8.8501e-03,\n",
              "          2.6978e-02,  1.9043e-02],\n",
              "        [ 4.5166e-02, -4.5166e-02, -3.9368e-03,  ...,  7.9590e-02,\n",
              "          7.2266e-02,  1.3000e-02]])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "with open(\"./data/word2vec/w2v_index_to_key.pkl\", \"rb\") as f:\n",
        "    w2v_google_index2key= pickle.load(f)\n",
        "\n",
        "with open(\"./data/word2vec/w2v_key_to_index.pkl\", \"rb\") as f:\n",
        "    w2v_google_key2index = pickle.load(f)\n",
        "\n",
        "w2v_embedding = torch.load(\"./data/word2vec/word2vec_embedding.pt\")\n",
        "\n",
        "\n",
        "display(w2v_google_index2key)\n",
        "display(w2v_google_key2index)\n",
        "display(w2v_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For every item in the dataset, I will take the average of the words in the Wikipedia Article "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'item': 'http://www.wikidata.org/entity/Q252187',\n",
              " 'name': 'áo dài',\n",
              " 'description': 'Vietnamese national costume, tunic',\n",
              " 'type': 'concept',\n",
              " 'category': 'fashion',\n",
              " 'subcategory': 'clothing',\n",
              " 'label': 'cultural representative'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('sapienzanlp/nlp2025_hw1_cultural_dataset')\n",
        "\n",
        "dataset['train'][332] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "from wikidata.client import Client\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "class WikiScraper:\n",
        "\n",
        "    def __init__(self, dataset, wikidata_client, output_path, load=False, load_file_path=None):\n",
        "\n",
        "        \"\"\"\n",
        "        Args: \n",
        "            dataset - Dataset that contains the informations we want \n",
        "            wikidata_client - wikidata.Client(), used to scrape the content in the WikiData pages\n",
        "            output_path - Path to the file that will be saved by the scraper e.g. /home/wikidata/ -> will generate /home/wikidata/wiki-text.txt and /home/wikidata/wikidict.pkl\n",
        "            load - skips the scraping and loads data from the file in load_file_path \n",
        "        \"\"\"\n",
        "        self.wikidat_client = wikidata_client\n",
        "        self.OUTPUT_PATH = output_path\n",
        "        self.dataset = dataset\n",
        "\n",
        "        \n",
        "        \n",
        "        self.entities_id = self.retrieve_entities()\n",
        "        self.entity2row = self.entity_to_row()\n",
        "\n",
        "        if(load == True):\n",
        "            os.makedirs(os.path.dirname(self.OUTPUT_PATH), exist_ok=True)\n",
        "            if(not load_file_path):\n",
        "                raise Exception(\"Load Path must be a valid path if load=True\")\n",
        "            with open(load_file_path, \"rb\") as f:\n",
        "\n",
        "                try:\n",
        "                    self.wikidict = pickle.load(f)\n",
        "                except: \n",
        "                    raise Exception(\"File cannot be loaded\")\n",
        "        else: \n",
        "            self.wikidict = self.retrieve_wikidict()\n",
        "\n",
        "\n",
        "    def extract_id(self, link):\n",
        "        \"\"\"\n",
        "        Retrieves the entity id from the \"item\" column in the dataset, which is a Wikidata link.\n",
        "\n",
        "            Args: \n",
        "            link - the link column\n",
        "        \"\"\"\n",
        "        return link.split(\"/\")[-1]\n",
        "\n",
        "    def retrieve_entities(self):\n",
        "\n",
        "        \"\"\"Retrieves all the Wikidata IDs from the dataset.\"\"\"\n",
        "        res = []\n",
        "        for row in self.dataset:\n",
        "            # item is the column relative to the wikidata link\n",
        "            item = row[\"item\"]\n",
        "            res.append(self.extract_id(item))\n",
        "        return res\n",
        "\n",
        "    def entity_to_row(self):\n",
        "        \"\"\"Creates the dictionary entity_id: row_number\"\"\"\n",
        "        dic={}\n",
        "        for index, row in enumerate(self.dataset):\n",
        "            item = row[\"item\"]\n",
        "            entity = self.extract_id(item)\n",
        "\n",
        "            dic[entity] = index \n",
        "\n",
        "        return dic\n",
        "\n",
        "    def retrieve_wikidict(self):\n",
        "        \"\"\"\n",
        "        Function that scrapes WikiData pages for the items in the dataset <br>\n",
        "        It does two things: <br>\n",
        "            1. returns a dictionary with the format\n",
        "            entity_id: wikipedia_article. <br>\n",
        "            2. it creates a file to OUTPUT_PATH containing all the text scraped from Wikipedia related articles\n",
        "\n",
        "            Args:\n",
        "            entities_id: list of entities id from Wikidata\n",
        "        \"\"\"\n",
        "        \n",
        "        entities_id = self.entities_id\n",
        "        # Wikidata client instantiation\n",
        "        client = Client()\n",
        "\n",
        "        # Useful sub-functions\n",
        "        def clean_wikipedia_extract(text):\n",
        "            \"\"\"Sub-function that cleans wikipedia text\"\"\"\n",
        "\n",
        "            # Remove unwanted paragraphs\n",
        "\n",
        "            text = re.sub(r\"^==.*?==\\s*\", \"\", text, flags=re.MULTILINE)\n",
        "\n",
        "            end_markers = [\"See also\", \"References\", \"External links\", \"Further reading\"]\n",
        "            for marker in end_markers:\n",
        "                pattern = rf\"==\\s*{marker}\\s*==.*\"\n",
        "                text = re.sub(pattern, \"\", text, flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "            text = text.replace(\"\\n\", \" \")\n",
        "            text = text.replace(\"\\t\", \" \")\n",
        "            return text.strip()\n",
        "\n",
        "        def get_text(item):\n",
        "            \"\"\"\n",
        "            Sub-function that handles the get request from Wikipedia\n",
        "\n",
        "            Arguments:\n",
        "            item -- wikidata.Entity\n",
        "            \"\"\"\n",
        "            sitelinks = item.data.get(\"sitelinks\", {})\n",
        "            enwiki = sitelinks.get(\"enwiki\")\n",
        "            if enwiki:\n",
        "                title = enwiki[\"title\"]\n",
        "\n",
        "                api_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "                params = {\n",
        "                    \"action\": \"query\",\n",
        "                    \"prop\": \"extracts\",\n",
        "                    \"explaintext\": True,\n",
        "                    \"titles\": title,\n",
        "                    \"format\": \"json\",\n",
        "                    \"redirects\": 1\n",
        "                }\n",
        "\n",
        "                res = requests.get(api_url, params=params).json()\n",
        "                pages = res.get(\"query\", {}).get(\"pages\", {})\n",
        "                if not pages:\n",
        "                    return \"\"\n",
        "                page = next(iter(pages.values()))\n",
        "                text = page.get(\"extract\", \"\")\n",
        "                text = text.lower()\n",
        "                text = clean_wikipedia_extract(text)\n",
        "                return text\n",
        "            else:\n",
        "                print(f\"No English Wikipedia page found for entity . (skipping)\")\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "        tot = len(entities_id)\n",
        "        dic = {}\n",
        "\n",
        "        # Save related Wikidata text to output_path \n",
        "        # and create the dictionary entity: wikipedia article\n",
        "        with open(self.OUTPUT_PATH+\"wiki-text.txt\", \"a\") as f:\n",
        "\n",
        "            for entity_id in tqdm(entities_id, total=tot):\n",
        "                item = client.get(entity_id, load = True)\n",
        "                text = get_text(item)\n",
        "                dic[entity_id] = text\n",
        "                text+=\"\\n\"\n",
        "                f.write(text)\n",
        "\n",
        "        with open(self.OUTPUT_PATH+\"wikidict.pkl\", \"wb\") as f:\n",
        "            pickle.dump(dic, f)\n",
        "\n",
        "        return dic\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'item': 'http://www.wikidata.org/entity/Q32786', 'name': '916', 'description': '2012 film by M. Mohanan', 'type': 'entity', 'category': 'films', 'subcategory': 'film', 'label': 'cultural exclusive'}\n",
            "{'item': 'http://www.wikidata.org/entity/Q371', 'name': '!!!', 'description': 'American dance-punk band from California', 'type': 'entity', 'category': 'music', 'subcategory': 'musical group', 'label': 'cultural representative'}\n"
          ]
        }
      ],
      "source": [
        "for i in dataset[\"train\"].select(range(2)):\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "wikiscraper = WikiScraper(dataset[\"train\"], Client(), \"./WikiScraper/\", load=True, load_file_path=\"./WikiScraper/wikidict.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train a Classifier for each item in the dataset. We take the Wikipedia article for the item and embed each word using Google's Word2Vec. For each article we take the Average and this will be passed to a fully connected layer that will output one of the three categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'item': 'http://www.wikidata.org/entity/Q371',\n",
              " 'name': '!!!',\n",
              " 'description': 'American dance-punk band from California',\n",
              " 'type': 'entity',\n",
              " 'category': 'music',\n",
              " 'subcategory': 'musical group',\n",
              " 'label': 'cultural representative'}"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:67: SyntaxWarning: invalid escape sequence '\\W'\n",
            "<>:67: SyntaxWarning: invalid escape sequence '\\W'\n",
            "/tmp/ipykernel_9189/3401684497.py:67: SyntaxWarning: invalid escape sequence '\\W'\n",
            "  def tokenize_words(self, text, pattern=\"\\W+\"):\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class WikiDataset(torch.utils.data.IterableDataset):\n",
        "\n",
        "    def __init__(self, dataset, wikidict, entity2row, w2v_key2index):\n",
        "\n",
        "        self.dataset = dataset \n",
        "        self.wikidict = wikidict\n",
        "        self.entity2row = entity2row\n",
        "        self.w2v_key2index = w2v_key2index \n",
        "    \n",
        "    def __iter__(self):\n",
        "\n",
        "        for row in self.dataset:\n",
        "            item = row[\"item\"]\n",
        "            entity_id = item.split(\"/\")[-1]\n",
        "            \n",
        "            article_words = self.tokenize_words(self.wikidict[entity_id])\n",
        "\n",
        "\n",
        "            encoded_words = self.encode_words(article_words)\n",
        "\n",
        "            encoded_words = torch.tensor(encoded_words)\n",
        "\n",
        "            label = row[\"label\"]\n",
        "\n",
        "            if(label == \"cultural representative\"):\n",
        "                label = 0 \n",
        "            elif(label == \"cultural agnostic\"):\n",
        "                label = 1\n",
        "            elif(label == \"cultural exclusive\"):\n",
        "                label = 2 \n",
        "            else: \n",
        "                continue\n",
        "            \n",
        "            one_hot = torch.tensor(label)\n",
        "\n",
        "            yield {\"input\": encoded_words, \"target\": one_hot}\n",
        "\n",
        "    \n",
        "    \n",
        "    # used for the DataLoader\n",
        "    # the sequences will all have the same length (length of the biggest sequence)\n",
        "    # the shorter ones will be padded with values 0 \n",
        "    def collate_fn(self, batch):\n",
        "        \n",
        "        token_lists = [item[\"input\"] for item in batch]\n",
        "        labels = [item[\"target\"] for item in batch]\n",
        "\n",
        "\n",
        "        padded_tokens = pad_sequence(token_lists, batch_first=True, padding_value=0)  # or your PAD token\n",
        "        labels = torch.stack(labels)\n",
        "\n",
        "        return {\"input\": padded_tokens, \"target\": labels }\n",
        "\n",
        "\n",
        "    def encode_words(self, words):\n",
        "\n",
        "        res = []\n",
        "        for word in words: \n",
        "            if(word and word in self.w2v_key2index):\n",
        "\n",
        "                res.append(self.w2v_key2index[word])\n",
        "        \n",
        "        return res \n",
        "\n",
        "    def tokenize_words(self, text, pattern=\"\\W+\"):\n",
        "        return [word for word in re.split(pattern, text.lower()) if word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "391"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wikidataset = WikiDataset(dataset[\"train\"], wikiscraper.wikidict, wikiscraper.entity2row, w2v_google_key2index)\n",
        "\n",
        "wikidataloader = DataLoader(wikidataset, batch_size = 16, collate_fn=wikidataset.collate_fn)\n",
        "\n",
        "total_batches = 0\n",
        "\n",
        "for i in wikidataloader:\n",
        "    total_batches+=1\n",
        "\n",
        "total_batches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CulturalClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, weights, embedding_size):\n",
        "\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
        "        self.output = nn.Linear(embedding_size, 3)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        try:\n",
        "            indices = input.long()\n",
        "            doc_mean = torch.mean(self.embedding(indices), dim=1)\n",
        "        except:\n",
        "            print(input)\n",
        "            raise Exception(\"invalid input\")\n",
        "        output = self.output(doc_mean)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "\n",
        "    def __init__(self, model, dataloader, optimizer, loss, device=\"cpu\"):\n",
        "\n",
        "        self.model = model \n",
        "        self.dataloader = dataloader \n",
        "        self.optimizer = optimizer \n",
        "        self.loss = loss\n",
        "        self.device = device\n",
        "        self.total_batches = self._calculate_batches()\n",
        "\n",
        "\n",
        "    def _calculate_batches(self):\n",
        "        \n",
        "        count = 0\n",
        "        for i in self.dataloader:\n",
        "            count+=1\n",
        "        return count\n",
        "\n",
        "    def train(self, epochs, save_checkpoints = False, output_folder = None, checkpoint_interval = 1):\n",
        "        \n",
        "        \n",
        "        if(save_checkpoints and output_folder == None ):\n",
        "           raise Exception(\"Path for output checkpoints needed\")\n",
        "\n",
        "        train_loss = 0.0\n",
        "        for epoch in range(epochs):\n",
        "            \n",
        "            epoch_loss = 0.0\n",
        "            num_batches = 0\n",
        "            for batch in tqdm(self.dataloader, total = total_batches):\n",
        "                \n",
        "                input, target = batch[\"input\"], batch[\"target\"]\n",
        "                input = input.to(self.device)\n",
        "                target = target.to(self.device)\n",
        "\n",
        "                output = self.model(input)\n",
        "\n",
        "\n",
        "                loss = self.loss(output, target)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                num_batches += 1\n",
        "            \n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            print(f'Epoch: {epoch} avg loss = {avg_loss:.4f}')\n",
        "\n",
        "            if(save_checkpoints):\n",
        "                if(epoch % checkpoint_interval==0):\n",
        "                    self.model.state_dict()\n",
        "                    torch.save(self.model.state_dict(), os.path.join(output_folder, f'state_{epoch}.pt'))\n",
        "            train_loss += avg_loss \n",
        "\n",
        "        avg_loss = train_loss / epochs \n",
        "\n",
        "\n",
        "        \n",
        "        return avg_loss \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1189"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "#del cultural_classifier\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "cultural_classifier = CulturalClassifier(w2v_embedding, 300)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "cultural_classifier.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(cultural_classifier.parameters(), lr=0.1)\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = cultural_classifier, \n",
        "    dataloader = wikidataloader,\n",
        "    optimizer = optimizer,\n",
        "    loss = loss,\n",
        "    device =  device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 90.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 avg loss = 1.0651\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 91.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 avg loss = 1.0454\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 91.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2 avg loss = 1.0357\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 91.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3 avg loss = 1.0293\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 92.36it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4 avg loss = 1.0244\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 90.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5 avg loss = 1.0202\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 91.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 6 avg loss = 1.0163\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 90.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 7 avg loss = 1.0128\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 92.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 8 avg loss = 1.0094\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 90.25it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 9 avg loss = 1.0061\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 91.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10 avg loss = 1.0030\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 90.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 11 avg loss = 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 90.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 12 avg loss = 0.9972\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 89.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 13 avg loss = 0.9944\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 90.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 14 avg loss = 0.9918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 91.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 15 avg loss = 0.9892\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 89.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 16 avg loss = 0.9868\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 91.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 17 avg loss = 0.9844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 91.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 18 avg loss = 0.9821\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 92.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 19 avg loss = 0.9799\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 92.05it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 20 avg loss = 0.9777\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 91.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 21 avg loss = 0.9756\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 91.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 22 avg loss = 0.9736\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 90.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 23 avg loss = 0.9716\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 91.81it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 24 avg loss = 0.9698\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 92.72it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 25 avg loss = 0.9679\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 92.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 26 avg loss = 0.9661\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 91.95it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 27 avg loss = 0.9644\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 92.32it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 28 avg loss = 0.9627\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 91.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 29 avg loss = 0.9611\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "avg_loss = trainer.train(30, output_folder=\"./data/cultural_classifier/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<gensim.models.keyedvectors.KeyedVectors at 0x7139b61858b0>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Create a new Word2Vec model with the same vocab\n",
        "trainable_model = Word2Vec(vector_size=300, min_count=1)\n",
        "\n",
        "# Build vocab from pretrained model\n",
        "trainable_model.build_vocab([list(word2vec_google.key_to_index.keys())])\n",
        "\n",
        "# Copy vectors\n",
        "trainable_model.wv.vectors = word2vec_google.vectors.copy()\n",
        "\n",
        "trainable_model.wv\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 1.1291504e-03, -8.9645386e-04,  3.1852722e-04, ...,\n",
              "        -1.5640259e-03, -1.2302399e-04, -8.6307526e-05],\n",
              "       [ 7.0312500e-02,  8.6914062e-02,  8.7890625e-02, ...,\n",
              "        -4.7607422e-02,  1.4465332e-02, -6.2500000e-02],\n",
              "       [-1.1779785e-02, -4.7363281e-02,  4.4677734e-02, ...,\n",
              "         7.1289062e-02, -3.4912109e-02,  2.4169922e-02],\n",
              "       ...,\n",
              "       [-1.9653320e-02, -9.0820312e-02, -1.9409180e-02, ...,\n",
              "        -1.6357422e-02, -1.3427734e-02,  4.6630859e-02],\n",
              "       [ 3.2714844e-02, -3.2226562e-02,  3.6132812e-02, ...,\n",
              "        -8.8500977e-03,  2.6977539e-02,  1.9042969e-02],\n",
              "       [ 4.5166016e-02, -4.5166016e-02, -3.9367676e-03, ...,\n",
              "         7.9589844e-02,  7.2265625e-02,  1.3000488e-02]], dtype=float32)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of distinct words: 268036\n",
            "Total occurrences of words in the dataset (excl. UNK tokens): 9180840\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(36817483, 787968484)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "VOCAB_SIZE_PRETRAINED, EMBEDDING_DIM_PRETRAINED = word2vec_google.vectors.shape\n",
        "key_to_index = word2vec_google.key_to_index\n",
        "\n",
        "train_set_pretrained = Word2VecDataset(\"./wiki-text.txt\", VOCAB_SIZE_PRETRAINED, \"UNK\", 5, pre_word2id=key_to_index)\n",
        "trainable_model.build_vocab(train_set_pretrained, update=True)  # Add new words\n",
        "trainable_model.train(train_set_pretrained, total_examples=trainable_model.corpus_count, epochs=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainable_model.save(\"fine_tuned_word2vec.model\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_model = Word2Vec(sentences=trainset_loader, vector_size = 300, window = 5, min_count = 1, sg=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "trainable_model.save(\"our-wiki.model\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "machine_learning_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
